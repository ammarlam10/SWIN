{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf46ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# !pip install deepspeed\n",
    "# !pip install --upgrade wandb\n",
    "import copy\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, List, Tuple\n",
    "\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import io, transforms\n",
    "# from torchvision.utils import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Wandb login:\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# import wandb\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value = user_secrets.get_secret(\"wandb_api_key\")\n",
    "# wandb.login(key=secret_value)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "# print(torch.__version__, pl.__version__, wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc30563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "# Image parameters\n",
    "TRAIN_FILES = \"/home/ammar/Desktop/LMU/ADL/data/C. Diabetic Retinopathy Grading/1. Original Images/a. Training Set\"\n",
    "IMAGE_SIZE = 256\n",
    "PATCH_SIZE = 16\n",
    "ZERO_PCT = 0.1\n",
    "PATCHES_PER_ROW = (IMAGE_SIZE // PATCH_SIZE)\n",
    "NUM_PATCHES = PATCHES_PER_ROW ** 2\n",
    "RGB_CHANNELS = 3\n",
    "NUM_PIXELS = PATCH_SIZE ** 2 * RGB_CHANNELS\n",
    "VALID_IMAGES = 5\n",
    "TOPK = 5\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "\n",
    "# Transformer parameters\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "\n",
    "# Update constants\n",
    "TEMPERATURE_S = 0.1\n",
    "TEMPERATURE_T = 0.05\n",
    "CENTER_MOMENTUM = 0.9\n",
    "TEACHER_MOMENTUM = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49994a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData(Dataset):\n",
    "    def __init__(self, files: List[str]):\n",
    "        self.files = files\n",
    "        self.randcrop_big = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0))\n",
    "        self.randcrop_small = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = io.read_image(self.files[i])\n",
    "        img1 = self.randcrop_big(img)\n",
    "        img2 = self.randcrop_small(img)\n",
    "        if img.shape[0] == 1:\n",
    "            img1 = torch.cat([img1]*3)\n",
    "            img2 = torch.cat([img2]*3)\n",
    "\n",
    "        return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1be5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "class CollateFn:\n",
    "    def reshape(self, batch):\n",
    "        patches = torch.stack(batch)\\\n",
    "                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\\\n",
    "                    .unfold(3, PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "        num_images = len(patches)\n",
    "        patches = patches.reshape(\n",
    "            num_images,\n",
    "            RGB_CHANNELS, \n",
    "            NUM_PATCHES, \n",
    "            PATCH_SIZE, \n",
    "            PATCH_SIZE\n",
    "        )\n",
    "        patches.transpose_(1, 2)\n",
    "        \n",
    "        return patches.reshape(num_images, NUM_PATCHES, -1) / 255.0 - 0.5\n",
    "        \n",
    "    def __call__(\n",
    "        self, batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        x1, x2 = zip(*batch)\n",
    "\n",
    "        return self.reshape(x1), self.reshape(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "917fa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class ImageOriginalData(Dataset):\n",
    "    def __init__(self, files: List[str]):\n",
    "        self.files = files\n",
    "        self.resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = io.read_image(self.files[i])\n",
    "        if img.shape[0] == 1:\n",
    "            img = torch.cat([img]*3)\n",
    "\n",
    "        return self.resize(img)\n",
    "    \n",
    "class CollateSingleImage(CollateFn):    \n",
    "    def __call__(\n",
    "        self, batch: List[torch.Tensor]\n",
    "    ) -> torch.FloatTensor:\n",
    "        return self.reshape(batch)\n",
    "    \n",
    "files = [str(file) for file in Path(TRAIN_FILES).glob(\"*.png\")]\n",
    "train_files, valid_files = train_test_split(files, test_size=0.15, random_state=42)\n",
    "\n",
    "train_data = ImageData(train_files)\n",
    "train_dl = DataLoader(\n",
    "    train_data, \n",
    "    BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateFn(),\n",
    ")\n",
    "\n",
    "valid_data = ImageOriginalData(valid_files)\n",
    "valid_dl = DataLoader(\n",
    "    valid_data, \n",
    "    BATCH_SIZE*2, \n",
    "    shuffle=False, \n",
    "    drop_last=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateSingleImage(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9473e90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([16, 256, 768]), torch.Size([16, 256, 768])),\n",
       " torch.Size([32, 256, 768]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "x, y = next(iter(train_dl))\n",
    "x2 = next(iter(valid_dl))\n",
    "(x.shape, y.shape), (x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e8919a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        # transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # positional embedding\n",
    "        w_pos = torch.randn(NUM_PATCHES, d_model) / d_model ** 0.5\n",
    "        cls_token = torch.randn(1, d_model) / d_model ** 0.5\n",
    "        self.register_parameter(\"pos_embed\", nn.Parameter(w_pos))\n",
    "        self.register_parameter(\"cls_token\", nn.Parameter(cls_token))\n",
    "\n",
    "        # pixel projection\n",
    "        self.linear = nn.Linear(2 * d_model, d_model)\n",
    "        self.norm1 = nn.LayerNorm(2 * d_model, elementwise_affine=False)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "        position = torch.stack([self.pos_embed] * batch_size)\n",
    "        x = torch.cat([x, position], dim=-1)\n",
    "        pixel_proj = self.norm2(F.relu(self.linear(self.norm1(x))))\n",
    "        batched_cls_token = torch.stack([self.cls_token]*batch_size)\n",
    "        cls_x = torch.cat([batched_cls_token, pixel_proj], dim=1)\n",
    "        \n",
    "        cls_x.transpose_(0, 1)\n",
    "        return F.normalize(self.encoder(cls_x)[0, ...], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fb7df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLoss:\n",
    "    def __init__(self, temperature_t: float, temperature_s: float):\n",
    "        self.temperature_t = temperature_t\n",
    "        self.temperature_s = temperature_s\n",
    "        \n",
    "    def __call__(\n",
    "        self, \n",
    "        t: torch.FloatTensor, \n",
    "        s: torch.FloatTensor, \n",
    "        center: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        t = F.softmax((t.detach() - center) / self.temperature_t, dim=1)\n",
    "        log_s = F.log_softmax(s / self.temperature_s, dim=1)\n",
    "\n",
    "        return -(t * log_s).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1839bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "def get_closest(embedding: torch.FloatTensor, i: int):\n",
    "    similarity = embedding @ embedding[i,:].T\n",
    "    scores, idx = similarity.topk(TOPK)\n",
    "    return scores.cpu().numpy(), idx.cpu().numpy()\n",
    "\n",
    "def get_closest_wandb_images(embedding: torch.FloatTensor, i: int, files: List[str]):\n",
    "    main_img = to_pil_image(resize(io.read_image(files[i])))\n",
    "    closest_imgs = [wandb.Image(main_img)]\n",
    "    \n",
    "    scores, idx = get_closest(embedding, i)\n",
    "    \n",
    "    for i, score in zip(idx, scores):\n",
    "        img = to_pil_image(resize(io.read_image(files[i])))\n",
    "        closest_imgs.append(\n",
    "            wandb.Image(img, caption=f\"{score:.4f}\")\n",
    "        )\n",
    "        \n",
    "    return closest_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1c2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher: nn.Module,\n",
    "        lr: float,\n",
    "        loss_fn: Callable,\n",
    "        valid_files: List[str],\n",
    "        dim: int,\n",
    "        center_momentum: float,\n",
    "        param_momentum: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = copy.deepcopy(teacher)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn\n",
    "        self.c_mom = center_momentum\n",
    "        self.p_mom = param_momentum\n",
    "        self.register_buffer(\"center\", torch.zeros((1, dim)).float())\n",
    "        self.valid_files = valid_files\n",
    "        \n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def loss_calculation(\n",
    "        self,\n",
    "        batch: Tuple[torch.FloatTensor, torch.FloatTensor],\n",
    "    ) -> torch.FloatTensor:\n",
    "        x1, x2 = batch\n",
    "        \n",
    "        s1, s2 = self.student(x1), self.student(x2)\n",
    "        t1, t2 = self.teacher(x1), self.teacher(x2)\n",
    "        \n",
    "        loss = self.loss_fn(t1, s2, self.center) + self.loss_fn(t2, s1, self.center)\n",
    "        \n",
    "        emperical_center = F.normalize(\n",
    "            torch.cat([t1, t2]).mean(dim=0, keepdims=True),\n",
    "            dim=-1,\n",
    "        )\n",
    "        return loss, emperical_center\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n",
    "    ) -> torch.Tensor:\n",
    "        loss, emperical_center = self.loss_calculation(batch)\n",
    "        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        self.center = F.normalize(\n",
    "            self.c_mom * self.center + (1 - self.c_mom) * emperical_center,\n",
    "            dim=-1,\n",
    "        )\n",
    "        for s_p, t_p in zip(self.student.parameters(), self.teacher.parameters()):\n",
    "            t_p.data = self.p_mom * t_p.data + (1 - self.p_mom) * s_p.data\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, images: torch.FloatTensor, *args: List[Any]) -> None:\n",
    "        return self.teacher(images)\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        valid_embeds = torch.cat([pred for pred in validation_step_outputs])\n",
    "        columns = [\"image\"] + [f\"closest_{i+1}\" for i in range(TOPK)]\n",
    "        indices = np.random.choice(len(self.valid_files), VALID_IMAGES, replace=False)\n",
    "        rows = [get_closest_wandb_images(valid_embeds, i, self.valid_files) for i in indices]\n",
    "        table = wandb.Table(data=rows, columns=columns)\n",
    "        self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n",
    "        \n",
    "    def on_after_backward(self):\n",
    "        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n",
    "            global_step = self.trainer.global_step\n",
    "            for name, param in self.student.named_parameters():\n",
    "                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n",
    "                    self.logger.experiment.log(\n",
    "                        {f\"{name}_grad\": wandb.Histogram(param.grad.cpu())}\n",
    "                    )\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return FusedAdam(self.student.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95c3f31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60f12bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /kaggle/working/logs\n",
    "teacher = Model(NUM_PIXELS, N_HEADS, N_LAYERS)\n",
    "h_loss = HLoss(TEMPERATURE_T, TEMPERATURE_S)\n",
    "lightning_model = LightningModel(\n",
    "    teacher, \n",
    "    LR,\n",
    "    h_loss,\n",
    "    valid_files,\n",
    "    NUM_PIXELS,\n",
    "    CENTER_MOMENTUM, \n",
    "    TEACHER_MOMENTUM,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b0e5758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:694: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "To use bfloat16 with native amp you must install torch greater or equal to 1.10.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# logger = WandbLogger(\"DINO\", \"/kaggle/working/logs/\", project=\"DINO\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_clip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;43;03m#     logger=logger,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_sanity_val_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:485\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:214\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_strategy()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# 5. Instantiate Precision Plugin\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_init_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init_strategy()\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:714\u001b[0m, in \u001b[0;36mAcceleratorConnector._check_and_init_precision\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, DDPFullyShardedStrategy):\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FullyShardedNativeMixedPrecisionPlugin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision_flag, device)\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNativeMixedPrecisionPlugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_precision_flag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_amp_type_flag \u001b[38;5;241m==\u001b[39m AMPType\u001b[38;5;241m.\u001b[39mAPEX:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_amp_level_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_amp_level_flag \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/adl/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py:49\u001b[0m, in \u001b[0;36mNativeMixedPrecisionPlugin.__init__\u001b[0;34m(self, precision, device, scaler)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TORCH_GREATER_EQUAL_1_10:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use bfloat16 with native amp you must install torch greater or equal to 1.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16\u001b[39m:\n\u001b[1;32m     53\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: To use bfloat16 with native amp you must install torch greater or equal to 1.10."
     ]
    }
   ],
   "source": [
    "# logger = WandbLogger(\"DINO\", \"/kaggle/working/logs/\", project=\"DINO\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    gpus=torch.cuda.device_count(),\n",
    "    gradient_clip_val=1.0,\n",
    "#     logger=logger,\n",
    "    precision=16,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d669a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lightning_model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd6e1b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fa192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
